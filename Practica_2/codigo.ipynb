{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirar como es el fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mr.fileids()) )#hay 2000 ficheros (1000 exemples positius i 1000 negatius)\n",
    "print(mr.words('pos/cv000_29590.txt'))\n",
    "print(mr.categories())\n",
    "print(mr.fileids('pos')) #lista todos los ficheros positivos\n",
    "print((list(mr.words('pos/cv000_29590.txt')), 'pos') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particion Train y Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pos = [mr.words(f) for f in mr.fileids('pos')]\n",
    "doc_neg = [mr.words(f) for f in mr.fileids('neg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(doc_pos)\n",
    "random.shuffle(doc_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = doc_pos[:int((len(doc_pos)*0.7))] #70% para el train\n",
    "test_pos = doc_pos[int((len(doc_pos)*0.7)):] #30% final para el test\n",
    "\n",
    "train_neg = doc_neg[:int((len(doc_neg)*0.7))]\n",
    "test_neg = doc_neg[int((len(doc_neg)*0.7)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_pos + train_neg\n",
    "test = test_pos + test_neg\n",
    "#hay que guardar estos ficheros porque cada vez que ejecutemos el codigo el random va a cambiar el contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_pos), len(train_neg))\n",
    "print(len(test_pos), len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [1]*700 + [0]*700\n",
    "print(len(train_labels))\n",
    "test_labels = [1]*300 + [0]*300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preproces(text):\n",
    "    text_clean = []\n",
    "    # Expresi√≥n regular para eliminar todo lo que no sea una letra o un espacio\n",
    "    regex = r\"[^a-zA-Z\\s]\"\n",
    "    \n",
    "    # Diccionario de reemplazo\n",
    "    replacements = {\n",
    "        \"t\": \"not\",\n",
    "        's': 'is', #aunque tambien puede ser posesivo\n",
    "        \"didn\": \"did\",\n",
    "        \"haven\": \"have\",\n",
    "        \"hasn\": \"has\",\n",
    "        \"hadn\": \"had\",\n",
    "        \"shouldn\": \"should\",\n",
    "        \"wouldn\": \"would\",\n",
    "        \"couldn\": \"could\",\n",
    "        \"mustn\": \"must\",\n",
    "        'doesn': 'does',\n",
    "        'isn' : 'is',\n",
    "        'aren' : 'are',\n",
    "        'll': 'will',\n",
    "        've' : 'have', \n",
    "        'd' : 'would',\n",
    "        'm': 'am'\n",
    "    }\n",
    "    \n",
    "    for w in text:\n",
    "        # Eliminar caracteres no deseados y n√∫meros\n",
    "        limpio = re.sub(regex, \"\", w)\n",
    "        # Convertir a min√∫sculas\n",
    "        limpio = limpio.lower()\n",
    "        # Eliminar espacios en blanco al principio y al final\n",
    "        limpio = limpio.strip()\n",
    "        # Eliminar si la palabra es \"s\"\n",
    "        #if limpio == \"s\":\n",
    "           # continue\n",
    "        # Reemplazar si est√° en el diccionario\n",
    "        if limpio in replacements:\n",
    "            limpio = replacements[limpio]\n",
    "        # Filtrar tokens vac√≠os\n",
    "        if limpio != \"\":\n",
    "            text_clean.append(limpio)\n",
    "    \n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = [preproces(words) for words in train]\n",
    "print(clean_train[0])\n",
    "clean_test = [preproces(words)for words in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poner tags y no incluir stop Words ya que no aportaran informacion importante para el Sentiment Analisys\n",
    "train_tagged = []\n",
    "tags = set()\n",
    "stop_tags = {'DT', 'IN', 'CC', 'PRP', 'PRP$', 'WP', 'WP$', 'RP', 'TO', 'CD', 'EX', 'WDT'}\n",
    "for l in clean_train:\n",
    "    aux = []\n",
    "    con_tag = nltk.pos_tag(l)\n",
    "    for w, tag in con_tag:\n",
    "        if tag not in stop_tags:\n",
    "            aux.append((w, tag))\n",
    "            tags.add(tag)\n",
    "    train_tagged.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tags) #todos los tags unicos que aparecen en el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejemplo del resultado\n",
    "print(train_tagged[0])\n",
    "print(train[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver otros ejemplos de palabras con un cierto tag\n",
    "palabras = []\n",
    "for l in train_tagged:\n",
    "    for palabra, etiqueta in l:\n",
    "            if etiqueta == '$':\n",
    "                palabras.append(palabra)\n",
    "print(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tag '$' es un error del tagger pero no upondr√° ning√∫n problema. Adem√°s, al observar el tag \"FW\" (palabras que se etiquetan como extranjeras) vemos que podr√≠amos eliminar errores, palabras en otros idiomas, nombres propios, pero tambi√©n palabras normales. Como posteriormnte se va a hacer filtrado de frequencias esperamos que se eliminar√°n las palabras no deseadas y quedar√°n las que s√≠ que tienen sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lematizan solo estas categorias, reconocidas por SentiWordNet:\n",
    "\n",
    "Sustantivos ('n')\t'NN', 'NNS', 'NNP', 'NNPS'\n",
    "Verbos ('v')\t'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "Adjetivos ('a')\t'JJ', 'JJR', 'JJS'\n",
    "Adverbios ('r')\t'RB', 'RBR', 'RBS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(p):\n",
    "  d = {'NN': 'n', 'NNS': 'n', 'NNP' : 'n', 'NNPS':'n', #sustantivos\n",
    "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a',  #adjetivos \n",
    "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v', 'MD':'v', #verbos\n",
    "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}  #adverbios\n",
    "  if p[1] in d:\n",
    "    return wnl.lemmatize(p[0], pos=d[p[1]])\n",
    "  return p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemat = []\n",
    "for l in train_tagged:\n",
    "    aux = []\n",
    "    for i in l:\n",
    "        aux.append(lemmatize(i))\n",
    "    train_lemat.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectroizaci√≥n BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_aplanado = [' '.join(sublista) for sublista in train_lemat]\n",
    "X_train = vectorizer.fit_transform(train_aplanado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aplanado[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminacion de frequencias bajas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumamos las frecuencias de cada palabra en todos los documentos\n",
    "word_frequencies = X_train.sum(axis=0).A1\n",
    "# Obtenemos el vocabulario mapeado a √≠ndices\n",
    "words = vectorizer.get_feature_names_out()\n",
    "# Convertimos en un diccionario {palabra: frecuencia}\n",
    "freq_dict = dict(zip(words, word_frequencies))\n",
    "# Ordenamos por frecuencia ascendente\n",
    "sorted_freq = sorted(freq_dict.items(), key=lambda x: x[1])\n",
    "print(sorted_freq[:10])  # Muestra las 10 palabras menos frecuentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ordenamos las frecuencias en orden descendente\n",
    "sorted_freq_values = np.array(sorted(word_frequencies, reverse=True))\n",
    "\n",
    "# Creamos la gr√°fica\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(sorted_freq_values, marker=\"o\", linestyle=\"-\")\n",
    "\n",
    "# Agregamos etiquetas\n",
    "plt.xlabel(\"Palabras ordenadas por frecuencia\")\n",
    "plt.ylabel(\"Frecuencia de aparici√≥n\")\n",
    "plt.title(\"Distribuci√≥n de frecuencias de palabras\")\n",
    "plt.yscale(\"log\") \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Histograma detallado con l√≠mite en eje X\n",
    "max_freq = int(max(word_frequencies))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(word_frequencies, bins=np.arange(1, max_freq + 1), edgecolor='black', log=True)\n",
    "\n",
    "plt.xlabel(\"Frecuencia exacta\")\n",
    "plt.ylabel(\"Cantidad de palabras\")\n",
    "plt.title(\"Histograma de frecuencias de palabras (zoom 0‚Äì100)\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq = int(max(word_frequencies))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(word_frequencies, bins=np.arange(1, max_freq + 1), edgecolor='black', log=True)\n",
    "\n",
    "plt.xlabel(\"Frecuencia exacta\")\n",
    "plt.ylabel(\"Cantidad de palabras\")\n",
    "plt.title(\"Histograma de frecuencias de palabras (zoom 0‚Äì100)\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# üîç Zoom en el eje X\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3  # Umbral m√≠nimo (si se pone mas agrande se eliminaran mas de la mitad de las observaciones)\n",
    "vectorizer = CountVectorizer(min_df=min_freq)\n",
    "X_filtered = vectorizer.fit_transform(train_aplanado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = X_filtered.sum(axis=0).A1\n",
    "sorted_freq_values = np.array(sorted(word_frequencies, reverse=True))\n",
    "\n",
    "# Creamos la gr√°fica\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(sorted_freq_values, marker=\"o\", linestyle=\"-\")\n",
    "\n",
    "# Agregamos etiquetas\n",
    "plt.xlabel(\"Palabras ordenadas por frecuencia\")\n",
    "plt.ylabel(\"Frecuencia de aparici√≥n filtarada\")\n",
    "plt.title(\"Distribuci√≥n de frecuencias de palabras\")\n",
    "plt.yscale(\"log\") \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.shape #train final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos supervisados. Parte 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que ya tengamos los datos preprocesados de manera que faciliten a los modelos a classificar los documentos, empezaremos a construir los modelos supervisados. \n",
    "\n",
    "Para esta pr√°ctica hemos querido comparar tres modelos supervisados significativamente diferentes entre ellos: KNN, Random Forest y SVM (Support Vector Machine). Para cada uno de los modelos se contemplar√° el mismo procedimiento de entrenamiento. \n",
    "\n",
    "Primero de todo, se usar√° la funci√≥n **GridSearchCV** para encontrar los mejores par√°metros que se ajusten a los datos del train, posteriormente se aplicar√° la funci√≥n cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "\n",
    "# Tus datos\n",
    "X = train_aplanado\n",
    "y = train_labels\n",
    "\n",
    "# M√©tricas que quieres evaluar\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'recall_macro': 'recall_macro'\n",
    "}\n",
    "\n",
    "# Resultados globales\n",
    "resultados = {}\n",
    "\n",
    "# ----------- MODELO 1: KNN -----------\n",
    "pipe_knn = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(min_df=3)),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_knn = {\n",
    "    'clf__n_neighbors': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(pipe_knn, param_grid=param_knn, cv=5, scoring='accuracy')\n",
    "grid_knn.fit(X, y)\n",
    "\n",
    "mejor_knn = grid_knn.best_estimator_\n",
    "\n",
    "# ----------- MODELO 2: Random Forest -----------\n",
    "pipe_rf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(min_df=3)),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_rf = {\n",
    "    'clf__n_estimators': [50, 100],\n",
    "    'clf__max_depth': [None, 10, 100]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(pipe_rf, param_grid=param_rf, cv=5, scoring='accuracy')\n",
    "grid_rf.fit(X, y)\n",
    "mejor_rf = grid_rf.best_estimator_\n",
    "\n",
    "# ----------- MODELO 3: SVM -----------\n",
    "pipe_svm = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(min_df=3)),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "param_svm = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(pipe_svm, param_grid=param_svm, cv=5, scoring='accuracy')\n",
    "grid_svm.fit(X, y)\n",
    "mejor_svm = grid_svm.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diccionario de modelos a comparar\n",
    "modelos = {\n",
    "    \"KNN\": mejor_knn,\n",
    "    \"RandomForest\": mejor_rf,\n",
    "    \"SVM\": mejor_svm\n",
    "}\n",
    "\n",
    "# Guardamos resultados\n",
    "resultados = {}\n",
    "\n",
    "# mejor_knn, mejor_rf, mejor_svm son Pipelines que contienen el mejor modelo y el vectorizador. \n",
    "for nombre, modelo in modelos.items():\n",
    "    cv_result = cross_validate(modelo, X, y, cv=5, scoring=scoring)\n",
    "\n",
    "    resumen = {m√©trica: round(cv_result[f'test_{m√©trica}'].mean(), 4) for m√©trica in scoring}\n",
    "    resultados[nombre] = resumen\n",
    "\n",
    "\n",
    "# Mostrar como DataFrame ordenado\n",
    "df_resultados = pd.DataFrame(resultados).T\n",
    "print(\"üîç Los mejores resultados de cada modelo:\")\n",
    "print(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos no supervisados. Parte 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from textserver import TextServer\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtar train_lemat\n",
    "\n",
    "word_freq = Counter()\n",
    "for doc in train_lemat:\n",
    "    word_freq.update(doc)\n",
    "\n",
    "min_freq = 5\n",
    "filtered_words = {word for word, freq in word_freq.items() if freq < min_freq}\n",
    "filtered_train_lemat = [\n",
    "    [word for word in doc if word not in filtered_words]\n",
    "    for doc in train_lemat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "ts = TextServer('taisiia.prymak', '', 'senses') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.senses('lively')[0][0]\n",
    "print(result)\n",
    "print(lesk(train_lemat[8], 'lively'))\n",
    "#hay palabras que Freeling es incapaz de encontrar su synset, pero lesk si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si Freeling no encuentra el synset intentaremos encontrarlo con Lesk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(w, contexto):\n",
    "    try:\n",
    "        result = ts.senses(w)[0][0]\n",
    "        categ = result[4][-1]\n",
    "        id = int(result[4][0:8])\n",
    "        synset1 = wn.synset_from_pos_and_offset(categ, id)\n",
    "        return synset1\n",
    "    except:\n",
    "        return lesk(contexto, w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_train_lemat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = [filtered_train_lemat[0]]\n",
    "print(prueba)\n",
    "etiquetas_predichas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo simple: Clasificaci¬¥on de polaridad: score(s) = poss ‚àí neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignaci√≥n de valores neutros\n",
    "\n",
    "Enfoque: Asignar un valor neutral (0) a todos los synsets no encontrados.\n",
    "\n",
    "Justificaci√≥n acad√©mica: Este enfoque asume que las palabras ausentes no contribuyen al sentimiento del texto. Puedes argumentar que SentiWordNet se enfoca en palabras con carga emocional clara, y por tanto, las palabras ausentes probablemente sean neutras en t√©rminos de sentimiento.\n",
    "\n",
    "Cita potencial: \"La ausencia de ciertos t√©rminos en l√©xicos de sentimiento como SentiWordNet puede interpretarse como evidencia de su naturaleza sem√°nticamente neutra en contextos evaluativos\" (Baccianella et al., 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#else: si el sinset NO esta en sentiWordNet se va a suponer que su puntuacion es 0 JUSTIFICAR \n",
    "#si es neutral clasificar como positivo?????????????????????????????????????????????????????????????????????\n",
    "\n",
    "def predecir_sentimiento(lista_docs):\n",
    "\n",
    "    for doc in lista_docs:\n",
    "        pos_s = 0\n",
    "        neg_s = 0\n",
    "\n",
    "        for w in doc:\n",
    "            s1 = get_synsets(w, doc)\n",
    "            if s1 is not None: #si ni freeling ni lesk ha encontrado el syntet la funcion get_synsets(w, doc) devuelve None\n",
    "                try:\n",
    "                    senti_synset = swn.senti_synset(s1.name())\n",
    "                    pos_s += senti_synset.pos_score()\n",
    "                    neg_s += senti_synset.neg_score()\n",
    "                except:\n",
    "                    pass  # si no est√° en SentiWordNet\n",
    "        etiquetas_predichas.append(1 if pos_s - neg_s >= 0 else 0)\n",
    "\n",
    "    return etiquetas_predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for l in tqdm(filtered_train_lemat):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define etiquetas √∫tiles para sentimiento\n",
    "POS_RELEVANTES = {'JJ', 'JJR', 'JJS',  # Adjetivos\n",
    "                  'NN', 'NNS', 'NNP', 'NNPS',  # Sustantivos\n",
    "                  'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbos\n",
    "                  'RB', 'RBR', 'RBS'}  # Adverbios\n",
    "\n",
    "SOLO_ADJETIVOS = {'JJ', 'JJR', 'JJS'}\n",
    "\n",
    "def filtrar_tagged_por_pos(tagged_docs, etiquetas_validas):\n",
    "    return [[w for w, tag in doc if tag in etiquetas_validas] for doc in tagged_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568794\n"
     ]
    }
   ],
   "source": [
    "docs_relevantes = filtrar_tagged_por_pos(train_tagged, POS_RELEVANTES)\n",
    "print(sum(len(doc) for doc in docs_relevantes))\n",
    "preds_relevantes = predecir_sentimiento(docs_relevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_adjetivos = filtrar_tagged_por_pos(train_tagged, SOLO_ADJETIVOS)\n",
    "preds_adjetivos = predecir_sentimiento(docs_adjetivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"üîç Evaluaci√≥n con POS relevantes:\")\n",
    "print(classification_report(train_labels, preds_relevantes))\n",
    "\n",
    "print(\"\\nüîç Evaluaci√≥n con solo adjetivos:\")\n",
    "print(classification_report(train_labels, preds_adjetivos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Evaluando LogisticRegression...\n",
      "üß™ Evaluando RandomForest...\n",
      "\n",
      "üìä Resultados promedio con cross-validation:\n",
      "                    accuracy  f1_macro  precision_macro  recall_macro\n",
      "LogisticRegression    0.6543    0.6540           0.6547        0.6543\n",
      "RandomForest          0.6371    0.6361           0.6390        0.6371\n",
      "\n",
      "üéØ Pesos finales entrenando con todo el dataset:\n",
      "\n",
      "Coeficientes (LogisticRegression):\n",
      "adj: 0.1895\n",
      "verb: 0.1000\n",
      "adv: 0.1356\n",
      "noun: 0.0084\n",
      "\n",
      "Importancia por feature (RandomForest):\n",
      "adj: 0.3100\n",
      "verb: 0.2252\n",
      "adv: 0.2365\n",
      "noun: 0.2283\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# POS relevantes\n",
    "POS_TAGS = {\n",
    "    'adj': {'JJ', 'JJR', 'JJS'},\n",
    "    'verb': {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'},\n",
    "    'adv': {'RB', 'RBR', 'RBS'},\n",
    "    'noun': {'NN', 'NNS', 'NNP', 'NNPS'}\n",
    "}\n",
    "\n",
    "def extract_pos_sentiment_features(tagged_docs):\n",
    "    X = []\n",
    "    for doc in tagged_docs:\n",
    "        scores = {'adj': 0, 'verb': 0, 'adv': 0, 'noun': 0}\n",
    "        tokens = [w for w, _ in doc]\n",
    "\n",
    "        for w, tag in doc:\n",
    "            pos_category = None\n",
    "            for key, tag_set in POS_TAGS.items():\n",
    "                if tag in tag_set:\n",
    "                    pos_category = key\n",
    "                    break\n",
    "            if pos_category:\n",
    "                syn = lesk(tokens, w)\n",
    "                if syn:\n",
    "                    try:\n",
    "                        senti = swn.senti_synset(syn.name())\n",
    "                        scores[pos_category] += senti.pos_score() - senti.neg_score()\n",
    "                    except:\n",
    "                        pass\n",
    "        X.append([scores['adj'], scores['verb'], scores['adv'], scores['noun']])\n",
    "    return np.array(X)\n",
    "\n",
    "# Extraemos features\n",
    "X_features = extract_pos_sentiment_features(train_tagged)\n",
    "y = train_labels\n",
    "\n",
    "# M√©tricas a evaluar\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'precision_macro': 'precision_macro',\n",
    "    'recall_macro': 'recall_macro'\n",
    "}\n",
    "\n",
    "# Modelos\n",
    "modelos = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Resultados\n",
    "resultados = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(f\"üß™ Evaluando {nombre}...\")\n",
    "    cv_result = cross_validate(modelo, X_features, y, cv=5, scoring=scoring)\n",
    "    resumen = {m√©trica: round(np.mean(cv_result[f'test_{m√©trica}']), 4) for m√©trica in scoring}\n",
    "    resultados[nombre] = resumen\n",
    "\n",
    "# Mostrar tabla de resultados\n",
    "df_resultados = pd.DataFrame(resultados).T\n",
    "print(\"\\nüìä Resultados promedio con cross-validation:\")\n",
    "print(df_resultados)\n",
    "\n",
    "# ENTRENAR FINAL para mostrar coeficientes / importancias\n",
    "print(\"\\nüéØ Pesos finales entrenando con todo el dataset:\")\n",
    "\n",
    "lr_model = modelos[\"LogisticRegression\"].fit(X_features, y)\n",
    "rf_model = modelos[\"RandomForest\"].fit(X_features, y)\n",
    "\n",
    "# Pesos LR\n",
    "print(\"\\nCoeficientes (LogisticRegression):\")\n",
    "for i, pos in enumerate(['adj', 'verb', 'adv', 'noun']):\n",
    "    print(f\"{pos}: {lr_model.coef_[0][i]:.4f}\")\n",
    "\n",
    "# Importancias RF\n",
    "print(\"\\nImportancia por feature (RandomForest):\")\n",
    "for i, pos in enumerate(['adj', 'verb', 'adv', 'noun']):\n",
    "    print(f\"{pos}: {rf_model.feature_importances_[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
